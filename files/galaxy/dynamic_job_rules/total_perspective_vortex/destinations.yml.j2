destinations:

  _slurm_common:
    abstract: true
    runner: slurm
    # these can be increased if we want to enable clamping
    max_accepted_cores: 24
    max_accepted_mem: 110
    # these are the clamp values, anything > max_* but < max_accepted_* will be clamped to these values
    max_cores: 24
    max_mem: 110
    context:
      time: "24:00:00"
    params:
      native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --mem={round(mem*1024)} --partition=normal"
      tmp_dir: true
    env:
      - name: LC_ALL
        value: C
      # are these necessary with tmp_dir set?
      - name: TMPDIR
        value: $_GALAXY_JOB_TMP_DIR
      - name: TMP
        value: $_GALAXY_JOB_TMP_DIR
      - name: TEMP
        value: $_GALAXY_JOB_TMP_DIR
      - name: _JAVA_OPTIONS
        value: $_JAVA_OPTIONS -Djava.io.tmpdir=$_GALAXY_JOB_TMP_DIR
      - name: SINGULARITYENV__JAVA_OPTIONS
        value: $_JAVA_OPTIONS

  slurm_conda:
    inherits: _slurm_common
    scheduling:
      require:
        - conda

  slurm_singularity:
    inherits: _slurm_common
    params:
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
    env:
      - name: SINGULARITYENV_LC_ALL
        value: $LC_ALL
    scheduling:
      accept:
        - general
        - slurm
      reject:
        - frontera

  frontera:
    runner: frontera
    max_accepted_cores: 112
    max_accepted_mem: 2048
    max_cores: 112
    max_mem: 2048
    context:
      time: "24:00:00"
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=small"
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: "{{ galaxy_config_dir }}/pulsar_frontera_actions.yml"
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
    env:
      - name: _JAVA_OPTIONS
        value: $_JAVA_OPTIONS -Xmx128g -Xms256m
      - name: LC_ALL
        value: C
      - name: TERM
        value: vt100
      - execute: ulimit -c 0
      - execute: ulimit -u 16384
      - name: SINGULARITYENV__JAVA_OPTIONS
        value: $_JAVA_OPTIONS -Djava.io.tmpdir=$_GALAXY_JOB_TMP_DIR
      - name: SINGULARITYENV_LC_ALL
        value: $LC_ALL
      - name: SINGULARITYENV_TERM
        value: $TERM
      - file: /etc/profile.d/z01_lmod.sh
      - execute: module load tacc-singularity
      - execute: module unload xalt
      - name: GALAXY_SLOTS
        value: "$SLURM_NTASKS"
      - name: GALAXY_MEMORY_MB
        value: "194560"
    rules:
      - if: mem > 190
        params:
          submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=nvdimm"
    scheduling:
      accept:
        - frontera
        - pulsar

---

#
# Job runner plugin configuration
#
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    workers: 2
  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    workers: 4
    drmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1
    invalidjobexception_retries: 5
    internalexception_retries: 5
  frontera:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: frontera
    amqp_url: "{{ pulsar_message_queue_url }}"
    galaxy_url: "https://{{ galaxy_instance_hostname }}"
    persistence_directory: "{{ galaxy_root }}/var/pulsar_amqp_ack"
    amqp_acknowledge: true
    amqp_ack_republish_time: 1200
    amqp_consumer_timeout: 2.0
    amqp_publish_retry: true
    amqp_publish_retry_max_retries: 60


handling:
  assign:
    - db-skip-locked
  max_grab: 16
  ready_window_size: 32

execution:
  default: slurm_normal_container
  environments:
    local:
      runner: local
    tpv_dispatcher:
      runner: dynamic
      type: python
      function: map_tool_to_destination
      rules_module: tpv.rules
      tpv_config_files:
        - https://raw.githubusercontent.com/galaxyproject/tpv-shared-database/main/tools.yml
        - "{{ galaxy_dynamic_job_rules_dir }}/total_perspective_vortex/default_tool.yml"
        - "{{ galaxy_dynamic_job_rules_dir }}/total_perspective_vortex/tools.yml"
        - "{{ galaxy_dynamic_job_rules_dir }}/total_perspective_vortex/destinations.yml"
    slurm:
      runner: slurm
      tmp_dir: True
      native_specification: --nodes=1 --ntasks=24 --ntasks-per-node=24 --mem=119896 --partition=normal
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
    frontera:
      runner: frontera
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=00:30:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    # single core, default (4GB) memory, non-container
    slurm_normal:
      runner: slurm
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, default (4GB) memory, singularity
    slurm_normal_container:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    slurm_normal_container_resolv_fix:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      singularity_run_extra_arguments: --no-mount=tmp
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # default multi core (6), default (4GB) memory, singularity
    slurm_normal_multicore_container:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, 8 GB memory, singularity
    slurm_normal_multicore_container_4c8g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=8000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (16GB) memory, singularity
    slurm_bigmem_multicore_container_4c16g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=16384 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (32GB) memory, singularity
    slurm_bigmem_multicore_container_4c32g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=32768 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (64GB) memory, singularity
    slurm_bigmem_multicore_container_4c64g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=65536 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (128GB) memory, singularity
    slurm_bigmem_multicore_container_4c128g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=131072 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (256GB) memory, singularity
    slurm_bigmem_multicore_container_4c256g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=257736 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, large (32GB) memory, singularity
    slurm_bigmem_container:
      runner: slurm
      native_specification: --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, default (4GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_normal_container_venv:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # default multi core (6), default (4GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_normal_multicore_container_venv:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # default multi core (6), default (64GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_bigmem_multicore_container_venv:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=65536 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # single core, default (4GB) memory, conda deps in singularity
    slurm_normal_container_conda:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, (8GB) memory, conda deps in singularity
    slurm_normal_container_conda_1c8g:
      runner: slurm
      native_specification: --ntasks=1 --mem=8000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # multi core, default (4GB) memory, conda deps in singularity
    slurm_normal_multicore_container_conda:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # Frontera via Pulsar
    frontera_development:
      runner: frontera
      submit_native_specification: --partition=development --nodes=1 --ntasks=56 --time=00:30:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    #### For testing
    frontera_small_2min:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=00:02:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_1hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=01:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_2hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=02:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_2668mb_2hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=02:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "2668"
    frontera_small_4hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=04:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_8hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=08:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_16hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=16:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_24hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=24:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_48hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=48:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_large_10min:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=00:10:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_1hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=01:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_2hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=02:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_4hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=04:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_8hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=08:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_16hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=16:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_24hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=24:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_36hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=56 --time=36:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_48hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=48:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"

tools:
  - id: upload1
    environment: slurm_normal
  - id: __DATA_FETCH__
    environment: slurm_normal
  - id: Cut1
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  - id: Remove beginning1
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  - id: addValue
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  # Other than the data source tools, these should still be runnable in a container
  - class: local
    environment: slurm_normal
  # Tools using a single core that need to run in the venv.
  - id: __SET_METADATA__
    environment: slurm_normal_container_venv
  - id: ucsc_table_direct1
    environment: slurm_normal_container_venv
  # Tools using multiple cores that need to run in the venv.
  - id: ncbi_makeblastdb
    environment: slurm_bigmem_multicore_container_venv
  # Tools that do not have corresponding BioContainers
  #### TPV Dispatcher
  - id: vsnp_build_tables
    environment: tpv_dispatcher
  - id: sample_names
    environment: tpv_dispatcher
  - id: seqkit_stats
    environment: tpv_dispatcher
  - id: seqkit_fx2tab
    environment: tpv_dispatcher
  - id: multiqc
    environment: tpv_dispatcher
  - id: raxml
    environment: tpv_dispatcher
  - id: bwa
    environment: tpv_dispatcher
  - id: bwa_mem
    environment: tpv_dispatcher
  - id: picard_MarkDuplicates
    environment: tpv_dispatcher
  - id: picard_RevertSam
    environment: tpv_dispatcher
  - id: picard_AddCommentsToBam
    environment: tpv_dispatcher
  - id: picard_AddOrReplaceReadGroups
    environment: tpv_dispatcher
  - id: picard_CollectInsertSizeMetrics
    environment: tpv_dispatcher
  - id: picard_BedToIntervalList
    environment: tpv_dispatcher
  - id: picard_CollectHsMetrics
    environment: tpv_dispatcher
  - id: picard_CollectRnaSeqMetrics
    environment: tpv_dispatcher
  - id: picard_artifact_metrics
    environment: tpv_dispatcher
  - id: picard_CollectWgsMetrics
    environment: tpv_dispatcher
  - id: picard_QualityScoreDistribution
    environment: tpv_dispatcher
  - id: picard_FilterSamReads
    environment: tpv_dispatcher
  - id: picard_DownsampleSam
    environment: tpv_dispatcher
  - id: picard_FixMateInformation
    environment: tpv_dispatcher
  - id: picard_MarkDuplicatesWithMateCigar
    environment: tpv_dispatcher
  - id: picard_SamToFastq
    environment: tpv_dispatcher
  - id: picard_ReorderSam
    environment: tpv_dispatcher
  - id: picard_MeanQualityByCycle
    environment: tpv_dispatcher
  - id: picard_CollectGcBiasMetrics
    environment: tpv_dispatcher
  - id: picard_SortSam
    environment: tpv_dispatcher
  - id: picard_FastqToSam
    environment: tpv_dispatcher
  - id: picard_CollectBaseDistributionByCycle
    environment: tpv_dispatcher
  - id: picard_ReplaceSamHeader
    environment: tpv_dispatcher
  - id: picard_ValidateSamFile
    environment: tpv_dispatcher
  - id: picard_CASM
    environment: tpv_dispatcher
  - id: picard_MergeSamFiles
    environment: tpv_dispatcher
  - id: picard_EstimateLibraryComplexity
    environment: tpv_dispatcher
  - id: picard_CleanSam
    environment: tpv_dispatcher
  - id: picard_RevertOriginalBaseQualitiesAndAddMateCigar
    environment: tpv_dispatcher
  - id: picard_NormalizeFasta
    environment: tpv_dispatcher
  - id: picard_MergeBamAlignment
    environment: tpv_dispatcher
  - id: picard_RevertSam
    environment: tpv_dispatcher
  - id: picard_AddCommentsToBam
    environment: tpv_dispatcher
  - id: picard_AddOrReplaceReadGroups
    environment: tpv_dispatcher
  - id: picard_CollectInsertSizeMetrics
    environment: tpv_dispatcher
  - id: picard_BedToIntervalList
    environment: tpv_dispatcher
  - id: picard_CollectHsMetrics
    environment: tpv_dispatcher
  - id: picard_CollectRnaSeqMetrics
    environment: tpv_dispatcher
  - id: picard_artifact_metrics
    environment: tpv_dispatcher
  - id: picard_CollectWgsMetrics
    environment: tpv_dispatcher
  - id: picard_QualityScoreDistribution
    environment: tpv_dispatcher
  - id: picard_FilterSamReads
    environment: tpv_dispatcher
  - id: picard_DownsampleSam
    environment: tpv_dispatcher
  - id: picard_FixMateInformation
    environment: tpv_dispatcher
  - id: freebayes
    environment: tpv_dispatcher
  - id: samtools_idxstats
    environment: tpv_dispatcher
  - id: samtools_sort
    environment: tpv_dispatcher
  - id: fastqc
    environment: tpv_dispatcher
  - id: bionano_scaffold
    environment: tpv_dispatcher
  - id: ctb_chemfp_nxn_clustering
    environment: tpv_dispatcher
  - id: ctb_chemfp_mol2fps
    environment: tpv_dispatcher
  - id: ctb_sdf2fps
    environment: tpv_dispatcher
  - id: deeptools_bam_compare
    environment: tpv_dispatcher
  - id: deeptools_bam_coverage
    environment: tpv_dispatcher
  - id: deeptools_compute_matrix
    environment: tpv_dispatcher
  - id: deeptools_plot_heatmap
    environment: tpv_dispatcher
  - id: salmon
    environment: tpv_dispatcher
  - id: tp_sorted_uniq
    environment: tpv_dispatcher
  - id: trim_galore
    environment: tpv_dispatcher
  - id: clustalw
    environment: tpv_dispatcher
  - id: cuffmerge
    environment: tpv_dispatcher
  - id: fastq_paired_end_joiner
    environment: tpv_dispatcher
  - id: cshl_fastq_quality_filter
    environment: tpv_dispatcher
  - id: kraken
    environment: tpv_dispatcher
  - id: ncbi_blastn_wrapper
    environment: tpv_dispatcher
  - id: ncbi_blastp_wrapper
    environment: tpv_dispatcher
  - id: ncbi_blastx_wrapper
    environment: tpv_dispatcher
  - id: ncbi_tblastn_wrapper
    environment: tpv_dispatcher
  - id: ncbi_tblastx_wrapper
    environment: tpv_dispatcher
  - id: samtool_filter2
    environment: tpv_dispatcher
  - id: samtools_mpileup
    environment: tpv_dispatcher
  - id: mummer_nucmer
    environment: tpv_dispatcher
  - id: maxquant
    environment: tpv_dispatcher
  - id: maxquant_mqpar
    environment: tpv_dispatcher
  - id: maxquant_mqpar
    environment: tpv_dispatcher
  - id: bcftools_mpileup
    environment: tpv_dispatcher
  - id: bcftools_norm
    environment: tpv_dispatcher
  - id: cat_bins
    environment: tpv_dispatcher
  - id: circos_bundlelinks
    environment: tpv_dispatcher
  - id: datamash_ops
    environment: tpv_dispatcher
  - id: deseq2
    environment: tpv_dispatcher
  - id: edger
    environment: tpv_dispatcher
  - id: fasta-stats
    environment: tpv_dispatcher
  - id: fastp
    environment: tpv_dispatcher
  - id: ggplot2_heatmap2
    environment: tpv_dispatcher
  - id: ggplot2_point
    environment: tpv_dispatcher
  - id: gubbins
    environment: tpv_dispatcher
  - id: jbrowse
    environment: tpv_dispatcher
  - id: kallisto_quant
    environment: tpv_dispatcher
  - id: kraken2
    environment: tpv_dispatcher
  - id: limma_voom
    environment: tpv_dispatcher
  - id: lofreq_call
    environment: tpv_dispatcher
  - id: lofreq_indelqual
    environment: tpv_dispatcher
  - id: macs2_bdgdiff
    environment: tpv_dispatcher
  - id: macs2_callpeak
    environment: tpv_dispatcher
  - id: merqury
    environment: tpv_dispatcher
  - id: mothur_align_seqs
    environment: tpv_dispatcher
  - id: mothur_chimera_vsearch
    environment: tpv_dispatcher
  - id: mothur_classify_otu
    environment: tpv_dispatcher
  - id: mothur_classify_seqs
    environment: tpv_dispatcher
  - id: mothur_count_seqs
    environment: tpv_dispatcher
  - id: mothur_dist_shared
    environment: tpv_dispatcher
  - id: mothur_filter_seqs
    environment: tpv_dispatcher
  - id: mothur_make_contigs
    environment: tpv_dispatcher
  - id: mothur_make_group
    environment: tpv_dispatcher
  - id: mothur_make_shared
    environment: tpv_dispatcher
  - id: mothur_merge_files
    environment: tpv_dispatcher
  - id: mothur_pre_cluster
    environment: tpv_dispatcher
  - id: mothur_screen_seqs
    environment: tpv_dispatcher
  - id: mothur_summary_seqs
    environment: tpv_dispatcher
  - id: mothur_taxonomy_to_krona
    environment: tpv_dispatcher
  - id: mothur_unique_seqs
    environment: tpv_dispatcher
  - id: nanoplot
    environment: tpv_dispatcher
  - id: iuc_pear
    environment: tpv_dispatcher
  - id: pilon
    environment: tpv_dispatcher
  - id: porechop
    environment: tpv_dispatcher
  - id: poretools_events
    environment: tpv_dispatcher
  - id: poretools_extract
    environment: tpv_dispatcher
  - id: poretools_hist
    environment: tpv_dispatcher
  - id: poretools_nucdist
    environment: tpv_dispatcher
  - id: poretools_occupancy
    environment: tpv_dispatcher
  - id: poretools_qualdist
    environment: tpv_dispatcher
  - id: poretools_qualpos
    environment: tpv_dispatcher
  - id: poretools_squiggle
    environment: tpv_dispatcher
  - id: poretools_stats
    environment: tpv_dispatcher
  - id: poretools_tabular
    environment: tpv_dispatcher
  - id: poretools_times
    environment: tpv_dispatcher
  - id: poretools_winner
    environment: tpv_dispatcher
  - id: poretools_yield_plot
    environment: tpv_dispatcher
  - id: qualimap_bamqc
    environment: tpv_dispatcher
  - id: rna_starsolo
    environment: tpv_dispatcher
  - id: roary
    environment: tpv_dispatcher
  - id: scanpy_cluster_reduce_dimension
    environment: tpv_dispatcher
  - id: scanpy_filter
    environment: tpv_dispatcher
  - id: scanpy_inspect
    environment: tpv_dispatcher
  - id: scanpy_normalize
    environment: tpv_dispatcher
  - id: scanpy_remove_confounders
    environment: tpv_dispatcher
  - id: shovill
    environment: tpv_dispatcher
  - id: snpEff
    environment: tpv_dispatcher
  - id: snpSift_annotate
    environment: tpv_dispatcher
  - id: fasterq_dump
    environment: tpv_dispatcher
  - id: fastq_dump
    environment: tpv_dispatcher
  - id: sam_dump
    environment: tpv_dispatcher
  - id: stringtie
    environment: tpv_dispatcher
  - id: stringtie_merge
    environment: tpv_dispatcher
  - id: cutadapt
    environment: tpv_dispatcher
  - id: trimmomatic
    environment: tpv_dispatcher
  - id: krona-text
    environment: tpv_dispatcher
  - id: augustus
    environment: tpv_dispatcher
  - id: deeptools_plot_fingerprint
    environment: tpv_dispatcher
  - id: flye
    environment: tpv_dispatcher
  - id: racon
    environment: tpv_dispatcher
  - id: repeatmasker_wrapper
    environment: tpv_dispatcher
  - id: gmx_sim
    environment: tpv_dispatcher
  - id: prokka
    environment: tpv_dispatcher
  - id: bowtie2
    environment: tpv_dispatcher
  - id: bwameth
    environment: tpv_dispatcher
  - id: fastq_groomer
    environment: tpv_dispatcher
  - id: fastq_paired_end_deinterlacer
    environment: tpv_dispatcher
  - id: fastq_paired_end_interlacer
    environment: tpv_dispatcher
  - id: lastz_wrapper_2
    environment: tpv_dispatcher
  - id: samtools_rmdup
    environment: tpv_dispatcher
  - id: samtools_fixmate
    environment: tpv_dispatcher
  - id: samtools_markdup
    environment: tpv_dispatcher
  - id: abricate
    environment: tpv_dispatcher
  - id: circos
    environment: tpv_dispatcher
  - id: hisat2
    environment: tpv_dispatcher
  - id: hyphy_gard
    environment: tpv_dispatcher
  - id: ivar_consensus
    environment: tpv_dispatcher
  - id: ivar_trim
    environment: tpv_dispatcher
  - id: ivar_variants
    environment: tpv_dispatcher
  - id: khmer_abundance_distribution_single
    environment: tpv_dispatcher
  - id: lofreq_filter
    environment: tpv_dispatcher
  - id: lofreq_viterbi
    environment: tpv_dispatcher
  - id: medaka_consensus_pipeline
    environment: tpv_dispatcher
  - id: medaka_variant
    environment: tpv_dispatcher
  - id: megahit
    environment: tpv_dispatcher
  - id: meryl
    environment: tpv_dispatcher
  - id: minimap2
    environment: tpv_dispatcher
  - id: rna_star
    environment: tpv_dispatcher
  - id: seqtk_seq
    environment: tpv_dispatcher
  - id: snippy
    environment: tpv_dispatcher
  - id: trinity
    environment: tpv_dispatcher
  - id: trinity_align_and_estimate_abundance
    environment: tpv_dispatcher
  - id: trinity_contig_exn50_statistic
    environment: tpv_dispatcher
  - id: trinity_abundance_estimates_to_matrix
    environment: tpv_dispatcher
  - id: trinity_gene_to_trans_map
    environment: tpv_dispatcher
  - id: trinity_run_de_analysis
    environment: tpv_dispatcher
  - id: trinity_samples_qccheck
    environment: tpv_dispatcher
  - id: trinity_super_transcripts
    environment: tpv_dispatcher
  - id: trinity_analyze_diff_expr
    environment: tpv_dispatcher
  - id: trinity_filter_low_expr_transcripts
    environment: tpv_dispatcher
  - id: describe_samples
    environment: tpv_dispatcher
  - id: trinity_define_clusters_by_cutting_tree
    environment: tpv_dispatcher
  - id: unicycler
    environment: tpv_dispatcher
  - id: htseq_count
    environment: tpv_dispatcher
  - id: rseqc_RPKM_saturation
    environment: tpv_dispatcher
  - id: rseqc_geneBody_coverage
    environment: tpv_dispatcher
  - id: rseqc_geneBody_coverage2
    environment: tpv_dispatcher
  - id: rseqc_read_distribution
    environment: tpv_dispatcher
  - id: metaspades
    environment: tpv_dispatcher
  - id: rbc_mafft
    environment: tpv_dispatcher
  - id: canu
    environment: tpv_dispatcher
  - id: hifiasm
    environment: tpv_dispatcher
  - id: busco
    environment: tpv_dispatcher
  - id: mothur_cluster_split
    environment: tpv_dispatcher
  - id: quast
    environment: tpv_dispatcher
  - id: spades
    environment: tpv_dispatcher
  - id: spades_rnaviralspades
    environment: tpv_dispatcher
  - id: rnaspades
    environment: tpv_dispatcher
  - id: spades_plasmidspades
    environment: tpv_dispatcher
  - id: spades_metaviralspades
    environment: tpv_dispatcher
  - id: spades_metaplasmidspades
    environment: tpv_dispatcher
  - id: spades_coronaspades
    environment: tpv_dispatcher
  - id: spades_biosyntheticspades
    environment: tpv_dispatcher
  - id: exonerate
    environment: tpv_dispatcher
  - id: peptide_shaker
    environment: tpv_dispatcher
  #### Frontera Small
  - id: bcftools_call
    environment: frontera_small_1hr
  - id: snippy_clean_full_aln
    environment: frontera_small_2hr
  - id: snippy_core
    environment: frontera_small_2hr
  - id: adapter_removal
    environment: frontera_small_2hr
  - id: vcffilter2
    environment: frontera_small_2hr
  - id: aldex2
    environment: frontera_small_4hr
  - id: ancombc
    environment: frontera_small_4hr
  - id: obi_illumina_pairend
    environment: frontera_small_4hr
  - id: obi_ngsfilter
    environment: frontera_small_4hr
  - id: obi_annotate
    environment: frontera_small_4hr
  - id: obi_clean
    environment: frontera_small_4hr
  - id: obi_convert
    environment: frontera_small_4hr
  - id: obi_sort
    environment: frontera_small_4hr
  - id: obi_stat
    environment: frontera_small_4hr
  - id: obi_tab
    environment: frontera_small_4hr
  - id: obi_uniq
    environment: frontera_small_4hr
  - id: blastxml_to_tabular
    environment: frontera_small_8hr
  - id: ncbi_blastdbcmd_info
    environment: frontera_small_8hr
  - id: ncbi_blastdbcmd_wrapper
    environment: frontera_small_8hr
  - id: ncbi_convert2blastmask_wrapper
    environment: frontera_small_8hr
  - id: ncbi_dustmasker_wrapper
    environment: frontera_small_8hr
  - id: ncbi_makeprofiledb
    environment: frontera_small_8hr
  - id: ncbi_rpsblast_wrapper
    environment: frontera_small_8hr
  - id: ncbi_rpstblastn_wrapper
    environment: frontera_small_8hr
  - id: ncbi_segmasker_wrapper
    environment: frontera_small_8hr
  - id: bg_diamond
    environment: frontera_small_8hr
  - id: bg_diamond_makedb
    environment: frontera_small_8hr
  - id: bbtools_bbmap
    environment: frontera_small_8hr
  - id: flash
    environment: frontera_small_8hr
  - id: bedtools_annotatebed
    environment: frontera_small_8hr
  - id: bedtools_closestbed
    environment: frontera_small_8hr
  - id: bedtools_clusterbed
    environment: frontera_small_8hr
  - id: bedtools_complementbed
    environment: frontera_small_8hr
  - id: bedtools_coveragebed
    environment: frontera_small_8hr
  - id: bedtools_expandbed
    environment: frontera_small_8hr
  - id: bedtools_fisher
    environment: frontera_small_8hr
  - id: bedtools_flankbed
    environment: frontera_small_8hr
  - id: bedtools_genomecoveragebed
    environment: frontera_small_8hr
  - id: bedtools_getfastabed
    environment: frontera_small_8hr
  - id: bedtools_groupbybed
    environment: frontera_small_8hr
  - id: bedtools_intersectbed
    environment: frontera_small_8hr
  - id: bedtools_jaccard
    environment: frontera_small_8hr
  - id: bedtools_links
    environment: frontera_small_8hr
  - id: bedtools_makewindowsbed
    environment: frontera_small_8hr
  - id: bedtools_map
    environment: frontera_small_8hr
  - id: bedtools_maskfastabed
    environment: frontera_small_8hr
  - id: bedtools_mergebed
    environment: frontera_small_8hr
  - id: bedtools_multicovtbed
    environment: frontera_small_8hr
  - id: bedtools_multiintersectbed
    environment: frontera_small_8hr
  - id: bedtools_nucbed
    environment: frontera_small_8hr
  - id: bedtools_overlapbed
    environment: frontera_small_8hr
  - id: bedtools_randombed
    environment: frontera_small_8hr
  - id: bedtools_reldistbed
    environment: frontera_small_8hr
  - id: bedtools_shufflebed
    environment: frontera_small_8hr
  - id: bedtools_slopbed
    environment: frontera_small_8hr
  - id: bedtools_sortbed
    environment: frontera_small_8hr
  - id: bedtools_spacingbed
    environment: frontera_small_8hr
  - id: bedtools_subtractbed
    environment: frontera_small_8hr
  - id: bedtools_tagbed
    environment: frontera_small_8hr
  - id: bedtools_unionbedgraph
    environment: frontera_small_8hr
  - id: bedtools_windowbed
    environment: frontera_small_8hr
  #### Frontera large
  - id: data_manager_dram_download
    environment: frontera_large_8hr
  - id: obi_grep
    environment: frontera_large_8hr
  - id: diamond_database_builder
    environment: frontera_large_8hr
  - id: megan_blast2lca
    environment: frontera_large_8hr
  - id: megan_blast2rma
    environment: frontera_large_8hr
  - id: megan_daa2info
    environment: frontera_large_8hr
  - id: megan_daa2rma
    environment: frontera_large_8hr
  - id: megan_daa_meganizer
    environment: frontera_large_8hr
  - id: megan_read_extractor
    environment: frontera_large_8hr
  - id: megan_sam2rma
    environment: frontera_large_8hr
  - id: sina
    environment: frontera_large_16hr
  - id: magicblast
    environment: frontera_large_16hr
  - id: concoct
    environment: frontera_large_24hr
  - id: malt_run
    environment: frontera_large_36hr
  #### Slurm normal multicore
  #### Slurm bigmem multicore
  - id: phyloseq_from_dada2
    environment: slurm_bigmem_multicore_container_4c16g
  - id: phyloseq_plot_richness
    environment: slurm_bigmem_multicore_container_4c16g
  - id: phyloseq_plot_ordination
    environment: slurm_bigmem_multicore_container_4c16g
  - id: bedtools_bamtobed
    environment: slurm_bigmem_multicore_container_4c32g
  - id: bedtools_bed12tobed6
    environment: slurm_bigmem_multicore_container_4c32g
  - id: bedtools_bedpetobam
    environment: slurm_bigmem_multicore_container_4c32g
  - id: bedtools_bedtobam
    environment: slurm_bigmem_multicore_container_4c32g
  - id: bedtools_bedtoigv
    environment: slurm_bigmem_multicore_container_4c32g
  - id: dada2_assignTaxonomyAddspecies
    environment: slurm_bigmem_multicore_container_4c32g
  - id: dada2_dada
    environment: slurm_bigmem_multicore_container_4c32g
  - id: dada2_learnErrors
    environment: slurm_bigmem_multicore_container_4c32g
  - id: qiime_align_seqs
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_alpha_diversity
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_alpha_rarefaction
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_assign_taxonomy
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_beta_diversity
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_beta_diversity_through_plots
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_collapse_samples
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_compare_categories
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_core_diversity
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_count_seqs
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_extract_barcodes
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_filter_alignment
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_filter_fasta
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_filter_otus_from_otu_table
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_filter_samples_from_otu_table
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_filter_taxa_from_otu_table
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_jackknifed_beta_diversity
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_make_emperor
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_make_otu_heatmap
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_make_otu_table
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_make_phylogeny
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_multiple_join_paired_ends
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_multiple_split_libraries_fastq
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_pick_closed_reference_otus
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_pick_open_reference_otus
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_pick_otus
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_pick_rep_set
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_plot_taxa_summary
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_split_libraries
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_split_libraries_fastq
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_summarize_taxa
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_summarize_taxa_through_plots
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_upgma_cluster
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_validate_mapping_file
    environment: slurm_bigmem_multicore_container_4c64g
  - id: bwa_mem_index_builder_data_manager
    environment: slurm_bigmem_multicore_container_4c64g

limits:
  - type: registered_user_concurrent_jobs
    value: 20
  # Per-user limits on queued/running jobs per-environment
  - type: environment_user_concurrent_jobs
    id: multicore
    value: 4
  # Total limits on queued/running jobs across all users
  - type: environment_total_concurrent_jobs
    id: frontera_development
    value: 1
  # Actual Frontera limits are 50 running and 200 pending
  - type: environment_total_concurrent_jobs
    id: frontera_limit
    value: 50

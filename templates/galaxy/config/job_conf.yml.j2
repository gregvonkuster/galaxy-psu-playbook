---

#
# Job runner plugin configuration
#
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    workers: 2
  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    workers: 4
    drmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1
    invalidjobexception_retries: 5
    internalexception_retries: 5
  frontera:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: frontera
    amqp_url: "{{ pulsar_message_queue_url }}"
    galaxy_url: "https://{{ galaxy_instance_hostname }}"
    persistence_directory: "{{ galaxy_root }}/var/pulsar_amqp_ack"
    amqp_acknowledge: true
    amqp_ack_republish_time: 1200
    amqp_consumer_timeout: 2.0
    amqp_publish_retry: true
    amqp_publish_retry_max_retries: 60


handling:
  assign:
    - db-skip-locked
  max_grab: 16
  ready_window_size: 32

execution:
  default: slurm_normal_container
  environments:
    local:
      runner: local
    # single core, default (4GB) memory, non-container
    slurm_normal:
      runner: slurm
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, default (4GB) memory, singularity
    slurm_normal_container:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    slurm_normal_container_resolv_fix:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      singularity_run_extra_arguments: --no-mount=tmp
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # default multi core (6), default (4GB) memory, singularity
    slurm_normal_multicore_container:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, 8 GB memory, singularity
    slurm_normal_multicore_container_4c8g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=8000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (16GB) memory, singularity
    slurm_bigmem_multicore_container_4c16g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=16384 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (32GB) memory, singularity
    slurm_bigmem_multicore_container_4c32g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=32768 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (64GB) memory, singularity
    slurm_bigmem_multicore_container_4c64g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=65536 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (128GB) memory, singularity
    slurm_bigmem_multicore_container_4c128g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=131072 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (256GB) memory, singularity
    slurm_bigmem_multicore_container_4c256g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=257736 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, large (32GB) memory, singularity
    slurm_bigmem_container:
      runner: slurm
      native_specification: --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, default (4GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_normal_container_venv:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # default multi core (6), default (4GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_normal_multicore_container_venv:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # single core, default (4GB) memory, conda deps in singularity
    slurm_normal_container_conda:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, (8GB) memory, conda deps in singularity
    slurm_normal_container_conda_1c8g:
      runner: slurm
      native_specification: --ntasks=1 --mem=8000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # multi core, default (4GB) memory, conda deps in singularity
    slurm_normal_multicore_container_conda:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # Frontera via Pulsar
    frontera_development:
      runner: frontera
      submit_native_specification: --partition=development --nodes=1 --ntasks=56 --time=00:30:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small:
      runner: frontera
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=48:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_large:
      runner: frontera
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=48:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_tmp:
      runner: frontera
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=00:02:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"


tools:
  - id: upload1
    environment: slurm_normal
  - id: __DATA_FETCH__
    environment: slurm_normal
  - id: Cut1
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  - id: addValue
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  # Other than the data source tools, these should still be runnable in a container
  - class: local
    # FIXME: /corral/main/projects/Galaxy-PSU/dev/galaxy/venv/bin/nodejs: error while loading shared libraries: libstdc++.so.6: cannot open shared object file: No such file or directory
    #environment: slurm_normal_container_venv
    environment: slurm_normal
  # Tools using a single core that need to run in the venv.
  - id: __SET_METADATA__
    environment: slurm_normal_container_venv
  # Tools that do not have corresponding BioContainers
  # Increase memory/cores
  - id: phyloseq_from_dada2
    environment: slurm_bigmem_multicore_container_4c16g
  - id: phyloseq_plot_richness
    environment: slurm_bigmem_multicore_container_4c16g
  - id: phyloseq_plot_ordination
    environment: slurm_bigmem_multicore_container_4c16g
  - id: pilon
    environment: slurm_bigmem_multicore_container_4c32g
  - id: flye
    environment: slurm_bigmem_multicore_container_4c32g
  - id: diamond_database_builder
    environment: slurm_bigmem_multicore_container_4c32g
  - id: bg_diamond
    environment: slurm_bigmem_multicore_container_4c32g
  - id: bwa
    environment: frontera_small
  - id: bwa_mem
    environment: frontera_small
  - id: bg_diamond_makedb
    environment: frontera_small
  - id: kraken2
    environment: frontera_small
  - id: bbtools_bbmap
    environment: frontera_small
  - id: magicblast
    environment: frontera_small
  - id: cat1
    environment: frontera_large_tmp
  - id: malt_run
    environment: frontera_large
  - id: megan_blast2lca
    environment: frontera_small
  - id: megan_blast2rma
    environment: frontera_small
  - id: megan_daa2info
    environment: frontera_small
  - id: megan_daa2rma
    environment: frontera_small
  - id: megan_daa_meganizer
    environment: frontera_small
  - id: megan_read_extractor
    environment: frontera_small
  - id: megan_sam2rma
    environment: frontera_small
  - id: vsnp_add_zero_coverage
    environment: frontera_small
  - id: vsnp_build_tables
    environment: frontera_small
  - id: vsnp_determine_ref_from_data
    environment: frontera_small
  - id: vsnp_get_snps
    environment: frontera_small
  - id: vsnp_statistics
    environment: frontera_small
  - id: spades_biosyntheticspades
    environment: frontera_small
  - id: metaspades
    environment: frontera_small
  - id: rnaspades
    environment: frontera_small
  - id: spades_coronaspades
    environment: frontera_small
  - id: spades_metaplasmidspades
    environment: frontera_small
  - id: spades_metaviralspades
    environment: frontera_small
  - id: spades_plasmidspades
    environment: frontera_small
  - id: spades_rnaviralspades
    environment: frontera_small
  - id: fastq_dump
    environment: slurm_normal_multicore_container
  - id: fasterq_dump
    environment: slurm_normal_multicore_container
  - id: sam_dump
    environment: slurm_normal_multicore_container
  - id: dada2_assignTaxonomyAddspecies
    environment: slurm_bigmem_multicore_container_4c32g
  - id: dada2_dada
    environment: slurm_bigmem_multicore_container_4c32g
  - id: dada2_learnErrors
    environment: slurm_bigmem_multicore_container_4c32g
  # Undeclared Python dep
  - id: blastxml_to_tabular
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_blastdbcmd_info
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_blastdbcmd_wrapper
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_blastn_wrapper
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_blastp_wrapper
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_blastx_wrapper
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_convert2blastmask_wrapper
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_dustmasker_wrapper
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_makeblastdb
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_makeprofiledb
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_rpsblast_wrapper
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_rpstblastn_wrapper
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_segmasker_wrapper
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_tblastn_wrapper
    environment: slurm_normal_multicore_container_venv
  - id: ncbi_tblastx_wrapper
    environment: slurm_normal_multicore_container_venv

limits:
  - type: registered_user_concurrent_jobs
    value: 20
  # Per-user limits on queued/running jobs per-environment
  - type: environment_user_concurrent_jobs
    id: multicore
    value: 4
  # Total limits on queued/running jobs across all users
  - type: environment_total_concurrent_jobs
    id: frontera_development
    value: 1
  - type: environment_total_concurrent_jobs
    id: frontera_small
    value: 200
  - type: environment_total_concurrent_jobs
    id: frontera_large
    value: 200

---

#
# Job runner plugin configuration
#
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    workers: 2
  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    workers: 4
    drmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1
    invalidjobexception_retries: 5
    internalexception_retries: 5
  frontera:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: frontera
    amqp_url: "{{ pulsar_message_queue_url }}"
    galaxy_url: "https://{{ galaxy_instance_hostname }}"
    persistence_directory: "{{ galaxy_root }}/var/pulsar_amqp_ack"
    amqp_acknowledge: true
    amqp_ack_republish_time: 1200
    amqp_consumer_timeout: 2.0
    amqp_publish_retry: true
    amqp_publish_retry_max_retries: 60


handling:
  assign:
    - db-skip-locked
  max_grab: 16
  ready_window_size: 32

execution:
  default: slurm_normal_container
  environments:
    local:
      runner: local
    # single core, default (4GB) memory, non-container
    slurm_normal:
      runner: slurm
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, default (4GB) memory, singularity
    slurm_normal_container:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    slurm_normal_container_resolv_fix:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      singularity_run_extra_arguments: --no-mount=tmp
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # default multi core (6), default (4GB) memory, singularity
    slurm_normal_multicore_container:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, 8 GB memory, singularity
    slurm_normal_multicore_container_4c8g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=8000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (16GB) memory, singularity
    slurm_bigmem_multicore_container_4c16g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=16384 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (32GB) memory, singularity
    slurm_bigmem_multicore_container_4c32g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=32768 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (64GB) memory, singularity
    slurm_bigmem_multicore_container_4c64g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=65536 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (128GB) memory, singularity
    slurm_bigmem_multicore_container_4c128g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=131072 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, large (256GB) memory, singularity
    slurm_bigmem_multicore_container_4c256g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=257736 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, large (32GB) memory, singularity
    slurm_bigmem_container:
      runner: slurm
      native_specification: --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, default (4GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_normal_container_venv:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # default multi core (6), default (4GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_normal_multicore_container_venv:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # default multi core (6), default (64GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_bigmem_multicore_container_venv:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=65536 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # single core, default (4GB) memory, conda deps in singularity
    slurm_normal_container_conda:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, (8GB) memory, conda deps in singularity
    slurm_normal_container_conda_1c8g:
      runner: slurm
      native_specification: --ntasks=1 --mem=8000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # multi core, default (4GB) memory, conda deps in singularity
    slurm_normal_multicore_container_conda:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # Frontera via Pulsar
    frontera_development:
      runner: frontera
      submit_native_specification: --partition=development --nodes=1 --ntasks=56 --time=00:30:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    #### For testing
    frontera_small_2min:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=00:02:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_1hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=01:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_2hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=02:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_4hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=04:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_8hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=08:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_16hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=16:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_24hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=24:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_48hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=48:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_large_2hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=02:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_4hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=04:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_8hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=08:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_16hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=16:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_24hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=24:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_36hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=36:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_48hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=48:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"

tools:
  - id: upload1
    environment: slurm_normal
  - id: __DATA_FETCH__
    environment: slurm_normal
  - id: Cut1
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  - id: addValue
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  # Other than the data source tools, these should still be runnable in a container
  - class: local
    environment: slurm_normal
  # Tools using a single core that need to run in the venv.
  - id: __SET_METADATA__
    environment: slurm_normal_container_venv
  - id: ucsc_table_direct1
    environment: slurm_normal_container_venv
  # Tools using multiple cores that need to run in the venv.
  - id: ncbi_makeblastdb
    environment: slurm_bigmem_multicore_container_venv
  # Tools that do not have corresponding BioContainers
  #### Frontera small
  - id: sample_names
    environment: frontera_small_2min
  - id: multiqc
    environment: frontera_small_1hr
  - id: raxml
    environment: frontera_small_1hr
  - id: vsnp_statistics
    environment: frontera_small_1hr
  - id: vsnp_determine_ref_from_data
    environment: frontera_small_2hr
  - id: vsnp_add_zero_coverage
    environment: frontera_small_2hr
  - id: snippy_clean_full_aln
    environment: frontera_small_2hr
  - id: snippy_core
    environment: frontera_small_2hr
  - id: snippy
    environment: frontera_small_2hr
  - id: adapter_removal
    environment: frontera_small_2hr
  - id: cutadapt
    environment: frontera_small_2hr
  - id: bwa_mem
    environment: frontera_small_2hr
  - id: picard_MarkDuplicates
    environment: frontera_small_2hr
  - id: freebayes
    environment: frontera_small_2hr
  - id: samtools_idxstats
    environment: frontera_small_2hr
  - id: vcffilter2
    environment: frontera_small_2hr
  - id: vsnp_build_tables
    environment: frontera_small_4hr
  - id: fastqc
    environment: frontera_small_8hr
  - id: blastxml_to_tabular
    environment: frontera_small_8hr
  - id: ncbi_blastdbcmd_info
    environment: frontera_small_8hr
  - id: ncbi_blastdbcmd_wrapper
    environment: frontera_small_8hr
  - id: ncbi_blastn_wrapper
    environment: frontera_small_8hr
  - id: ncbi_blastp_wrapper
    environment: frontera_small_8hr
  - id: ncbi_blastx_wrapper
    environment: frontera_small_8hr
  - id: ncbi_convert2blastmask_wrapper
    environment: frontera_small_8hr
  - id: ncbi_dustmasker_wrapper
    environment: frontera_small_8hr
  - id: ncbi_makeprofiledb
    environment: frontera_small_8hr
  - id: ncbi_rpsblast_wrapper
    environment: frontera_small_8hr
  - id: ncbi_rpstblastn_wrapper
    environment: frontera_small_8hr
  - id: ncbi_segmasker_wrapper
    environment: frontera_small_8hr
  - id: ncbi_tblastn_wrapper
    environment: frontera_small_8hr
  - id: ncbi_tblastx_wrapper
    environment: frontera_small_8hr
  - id: bg_diamond
    environment: frontera_small_8hr
  - id: flye
    environment: frontera_small_8hr
  - id: bwa
    environment: frontera_small_8hr
  - id: bg_diamond_makedb
    environment: frontera_small_8hr
  - id: kraken2
    environment: frontera_small_8hr
  - id: bbtools_bbmap
    environment: frontera_small_8hr
  - id: flash
    environment: frontera_small_8hr
  - id: bedtools_annotatebed
    environment: frontera_small_8hr
  - id: bedtools_closestbed
    environment: frontera_small_8hr
  - id: bedtools_clusterbed
    environment: frontera_small_8hr
  - id: bedtools_complementbed
    environment: frontera_small_8hr
  - id: bedtools_coveragebed
    environment: frontera_small_8hr
  - id: bedtools_expandbed
    environment: frontera_small_8hr
  - id: bedtools_fisher
    environment: frontera_small_8hr
  - id: bedtools_flankbed
    environment: frontera_small_8hr
  - id: bedtools_genomecoveragebed
    environment: frontera_small_8hr
  - id: bedtools_getfastabed
    environment: frontera_small_8hr
  - id: bedtools_groupbybed
    environment: frontera_small_8hr
  - id: bedtools_intersectbed
    environment: frontera_small_8hr
  - id: bedtools_jaccard
    environment: frontera_small_8hr
  - id: bedtools_links
    environment: frontera_small_8hr
  - id: bedtools_makewindowsbed
    environment: frontera_small_8hr
  - id: bedtools_map
    environment: frontera_small_8hr
  - id: bedtools_maskfastabed
    environment: frontera_small_8hr
  - id: bedtools_mergebed
    environment: frontera_small_8hr
  - id: bedtools_multicovtbed
    environment: frontera_small_8hr
  - id: bedtools_multiintersectbed
    environment: frontera_small_8hr
  - id: bedtools_nucbed
    environment: frontera_small_8hr
  - id: bedtools_overlapbed
    environment: frontera_small_8hr
  - id: bedtools_randombed
    environment: frontera_small_8hr
  - id: bedtools_reldistbed
    environment: frontera_small_8hr
  - id: bedtools_shufflebed
    environment: frontera_small_8hr
  - id: bedtools_slopbed
    environment: frontera_small_8hr
  - id: bedtools_sortbed
    environment: frontera_small_8hr
  - id: bedtools_spacingbed
    environment: frontera_small_8hr
  - id: bedtools_subtractbed
    environment: frontera_small_8hr
  - id: bedtools_tagbed
    environment: frontera_small_8hr
  - id: bedtools_unionbedgraph
    environment: frontera_small_8hr
  - id: bedtools_windowbed
    environment: frontera_small_8hr
  #### Frontera large
  - id: vsnp_get_snps
    environment: frontera_large_8hr
  - id: trimmomatic
    environment: frontera_large_8hr
  - id: diamond_database_builder
    environment: frontera_large_8hr
  - id: pilon
    environment: frontera_large_8hr
  - id: unicycler
    environment: frontera_large_8hr
  - id: megan_blast2lca
    environment: frontera_large_8hr
  - id: megan_blast2rma
    environment: frontera_large_8hr
  - id: megan_daa2info
    environment: frontera_large_8hr
  - id: megan_daa2rma
    environment: frontera_large_8hr
  - id: megan_daa_meganizer
    environment: frontera_large_8hr
  - id: megan_read_extractor
    environment: frontera_large_8hr
  - id: megan_sam2rma
    environment: frontera_large_8hr
  - id: magicblast
    environment: frontera_large_16hr
  - id: concoct
    environment: frontera_large_24hr
  - id: spades_biosyntheticspades
    environment: frontera_large_24hr
  - id: metaspades
    environment: frontera_large_24hr
  - id: rnaspades
    environment: frontera_large_24hr
  - id: spades_coronaspades
    environment: frontera_large_24hr
  - id: spades_metaplasmidspades
    environment: frontera_large_24hr
  - id: spades_metaviralspades
    environment: frontera_large_24hr
  - id: spades_plasmidspades
    environment: frontera_large_24hr
  - id: spades_rnaviralspades
    environment: frontera_large_24hr
  - id: malt_run
    environment: frontera_large_36hr
  - id: trinity_contig_exn50_statistic
    environment: frontera_large_36hr
  - id: trinity_abundance_estimates_to_matrix
    environment: frontera_large_36hr
  - id: trinity_gene_to_trans_map
    environment: frontera_large_36hr
  - id: trinity_run_de_analysis
    environment: frontera_large_36hr
  - id: trinity_samples_qccheck
    environment: frontera_large_36hr
  - id: trinity_super_transcripts
    environment: frontera_large_36hr
  - id: trinity_analyze_diff_expr
    environment: frontera_large_36hr
  - id: trinity_filter_low_expr_transcripts
    environment: frontera_large_36hr
  - id: trinity_align_and_estimate_abundance
    environment: frontera_large_36hr
  - id: describe_samples
    environment: frontera_large_36hr
  - id: trinity
    environment: frontera_large_36hr
  - id: trinity_define_clusters_by_cutting_tree
    environment: frontera_large_36hr
  #### Slurm normal multicore
  - id: fastq_dump
    environment: slurm_normal_multicore_container
  - id: fasterq_dump
    environment: slurm_normal_multicore_container
  - id: sam_dump
    environment: slurm_normal_multicore_container
  #### Slurm bigmem multicore
  - id: phyloseq_from_dada2
    environment: slurm_bigmem_multicore_container_4c16g
  - id: phyloseq_plot_richness
    environment: slurm_bigmem_multicore_container_4c16g
  - id: phyloseq_plot_ordination
    environment: slurm_bigmem_multicore_container_4c16g
  - id: bedtools_bamtobed
    environment: slurm_bigmem_multicore_container_4c32g
  - id: bedtools_bed12tobed6
    environment: slurm_bigmem_multicore_container_4c32g
  - id: bedtools_bedpetobam
    environment: slurm_bigmem_multicore_container_4c32g
  - id: bedtools_bedtobam
    environment: slurm_bigmem_multicore_container_4c32g
  - id: bedtools_bedtoigv
    environment: slurm_bigmem_multicore_container_4c32g
  - id: dada2_assignTaxonomyAddspecies
    environment: slurm_bigmem_multicore_container_4c32g
  - id: dada2_dada
    environment: slurm_bigmem_multicore_container_4c32g
  - id: dada2_learnErrors
    environment: slurm_bigmem_multicore_container_4c32g
  - id: qiime_align_seqs
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_alpha_diversity
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_alpha_rarefaction
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_assign_taxonomy
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_beta_diversity
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_beta_diversity_through_plots
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_collapse_samples
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_compare_categories
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_core_diversity
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_count_seqs
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_extract_barcodes
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_filter_alignment
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_filter_fasta
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_filter_otus_from_otu_table
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_filter_samples_from_otu_table
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_filter_taxa_from_otu_table
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_jackknifed_beta_diversity
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_make_emperor
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_make_otu_heatmap
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_make_otu_table
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_make_phylogeny
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_multiple_join_paired_ends
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_multiple_split_libraries_fastq
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_pick_closed_reference_otus
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_pick_open_reference_otus
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_pick_otus
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_pick_rep_set
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_plot_taxa_summary
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_split_libraries
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_split_libraries_fastq
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_summarize_taxa
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_summarize_taxa_through_plots
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_upgma_cluster
    environment: slurm_bigmem_multicore_container_4c64g
  - id: qiime_validate_mapping_file
    environment: slurm_bigmem_multicore_container_4c64g
  - id: bwa_mem_index_builder_data_manager
    environment: slurm_bigmem_multicore_container_4c64g

limits:
  - type: registered_user_concurrent_jobs
    value: 20
  # Per-user limits on queued/running jobs per-environment
  - type: environment_user_concurrent_jobs
    id: multicore
    value: 4
  # Total limits on queued/running jobs across all users
  - type: environment_total_concurrent_jobs
    id: frontera_development
    value: 1
  # Actual Frontera limits are 50 running and 200 pending
  - type: environment_total_concurrent_jobs
    id: frontera_limit
    value: 50

---

#
# Job runner plugin configuration
#
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    workers: 2
  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    workers: 4
    drmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1
    invalidjobexception_retries: 5
    internalexception_retries: 5
  frontera:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: frontera
    amqp_url: "{{ pulsar_message_queue_url }}"
    galaxy_url: "https://{{ galaxy_instance_hostname }}"
    persistence_directory: "{{ galaxy_root }}/var/pulsar_amqp_ack"
    amqp_acknowledge: true
    amqp_ack_republish_time: 1200
    amqp_consumer_timeout: 2.0
    amqp_publish_retry: true
    amqp_publish_retry_max_retries: 60


handling:
  assign:
    - db-skip-locked
  max_grab: 16
  ready_window_size: 32

execution:
  default: tpv_dispatcher
  environments:
    local:
      runner: local
    tpv_dispatcher:
      runner: dynamic
      type: python
      function: map_tool_to_destination
      rules_module: tpv.rules
      tpv_config_files:
        - https://raw.githubusercontent.com/galaxyproject/tpv-shared-database/main/tools.yml
        - "{{ galaxy_dynamic_job_rules_dir }}/total_perspective_vortex/default_tool.yml"
        - "{{ galaxy_dynamic_job_rules_dir }}/total_perspective_vortex/tools.yml"
        - "{{ galaxy_dynamic_job_rules_dir }}/total_perspective_vortex/destinations.yml"
    slurm:
      runner: slurm
      tmp_dir: True
      native_specification: --nodes=1 --ntasks=24 --ntasks-per-node=24 --mem=119896 --partition=normal
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
    frontera:
      runner: frontera
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=00:30:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    # single core, default (4GB) memory, non-container
    slurm_normal:
      runner: slurm
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, default (4GB) memory, singularity
    slurm_normal_container:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    slurm_normal_container_resolv_fix:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      singularity_run_extra_arguments: --no-mount=tmp
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # default multi core (6), default (4GB) memory, singularity
    slurm_normal_multicore_container:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # 4 cores, 8 GB memory, singularity
    slurm_normal_multicore_container_4c8g:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=4 --mem=8000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, large (32GB) memory, singularity
    slurm_bigmem_container:
      runner: slurm
      native_specification: --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, default (4GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_normal_container_venv:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # default multi core (6), default (4GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_normal_multicore_container_venv:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # default multi core (6), default (64GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_bigmem_multicore_container_venv:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=65536 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # single core, default (4GB) memory, conda deps in singularity
    slurm_normal_container_conda:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # single core, (8GB) memory, conda deps in singularity
    slurm_normal_container_conda_1c8g:
      runner: slurm
      native_specification: --ntasks=1 --mem=8000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # multi core, default (4GB) memory, conda deps in singularity
    slurm_normal_multicore_container_conda:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
    # Frontera via Pulsar
    frontera_development:
      runner: frontera
      submit_native_specification: --partition=development --nodes=1 --ntasks=56 --time=00:30:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    #### For testing
    frontera_small_2min:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=00:02:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_2hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=02:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_4hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=04:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_small_8hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=small --nodes=1 --ntasks=56 --time=08:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx128g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "194560"
    frontera_large_10min:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=00:10:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - execute: ulimit -u 16384
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_2hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=02:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"
    frontera_large_8hr:
      runner: frontera
      tags: [frontera_limit]
      submit_native_specification: --partition=nvdimm --nodes=1 --ntasks=112 --time=08:00:00
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      file_action_config: {{ galaxy_config_dir }}/pulsar_frontera_actions.yml
      outputs_to_working_directory: false
      jobs_directory: "/scratch1/08500/{{ tacc_user }}/staging"
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx1792g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        - execute: module unload xalt
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: GALAXY_MEMORY_MB
          value: "1966080"

tools:
  # TODO: figure out how to make sure these do not run on Frontera.
  # param_value_from_file
  #__FILTER_FAILED_DATASETS__
  - id: upload1
    environment: slurm_normal
  - id: __DATA_FETCH__
    environment: slurm_normal
  - id: Cut1
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  - id: Remove beginning1
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  - id: addValue
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  # Other than the data source tools, these should still be runnable in a container
  - class: local
    environment: slurm_normal
  # Tools using a single core that need to run in the venv.
  - id: __SET_METADATA__
    environment: slurm_normal_container_venv
  - id: ucsc_table_direct1
    environment: slurm_normal_container_venv
  # Tools using multiple cores that need to run in the venv.
  - id: ncbi_makeblastdb
    environment: slurm_bigmem_multicore_container_venv
  # Tools that do not have corresponding BioContainers
  #### TPV Dispatcher
  - id: vsnp_build_tables
    environment: frontera_large_8hr
  - id: vsnp_statistics
    environment: frontera_small_2hr
  - id: vsnp_add_zero_coverage
    environment: frontera_small_2hr
  - id: vsnp_get_snps
    environment: frontera_large_8hr
  - id: sample_names
    environment: frontera_large_10min
  - id: seqkit_fx2tab
    environment: frontera_small_2hr
  - id: multiqc
    environment: frontera_small_4hr
  - id: raxml
    environment: frontera_large_8hr
  - id: bwa_mem
    environment: frontera_small_2hr
  - id: picard_MarkDuplicates
    environment: frontera_small_8hr
  - id: freebayes
    environment: frontera_small_8hr
  - id: samtools_fixmate
    environment: frontera_small_2hr
  - id: vcffilter2
    environment: frontera_small_4hr

limits:
  - type: registered_user_concurrent_jobs
    value: 20
  # Per-user limits on queued/running jobs per-environment
  - type: environment_user_concurrent_jobs
    id: multicore
    value: 4
  # Total limits on queued/running jobs across all users
  - type: environment_total_concurrent_jobs
    id: frontera_development
    value: 1
  # Actual Frontera limits are 50 running and 200 pending
  - type: environment_total_concurrent_jobs
    id: frontera_limit
    value: 50

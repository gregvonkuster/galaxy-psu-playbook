---

#
# Job runner plugin configuration
#
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    workers: 2
  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    workers: 4
    drmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1
    invalidjobexception_retries: 5
    internalexception_retries: 5

handling:
  assign:
    - db-skip-locked
  max_grab: 16
  ready_window_size: 32

execution:
  default: slurm_normal_container
  environments:
    local:
      runner: local
    # single core, default (4GB) memory, non-container
    slurm_normal:
      runner: slurm
      env:
        - name: LC_ALL
          value: C
    # single core, default (4GB) memory, singularity
    slurm_normal_container:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
    slurm_normal_container_resolv_fix:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      singularity_run_extra_arguments: --no-mount=tmp
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
    # multi core, default (4GB) memory, singularity
    slurm_normal_multicore_container:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
    # single core, large (32GB) memory, singularity
    slurm_bigmem_container:
      runner: slurm
      native_specification: --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
    # single core, default (4GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_normal_container_venv:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # single core, default (4GB) memory, conda deps in singularity
    slurm_normal_container_conda:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
    # multi core, default (4GB) memory, conda deps in singularity
    slurm_normal_multicore_container_conda:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=4000
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C

tools:
  - id: upload1
    environment: slurm_normal
  - id: __DATA_FETCH__
    environment: slurm_normal
  # Other than the data source tools, these should still be runnable in a container
  - class: local
    # FIXME: /corral/main/projects/Galaxy-PSU/dev/galaxy/venv/bin/nodejs: error while loading shared libraries: libstdc++.so.6: cannot open shared object file: No such file or directory
    #environment: slurm_normal_container_venv
    environment: slurm_normal
  # TODO: The containers for these tools have the resolv.conf issue and should be rebuilt. See: https://github.com/bioconda/bioconda-recipes/issues/11583
  # mulled-v2-a123a99e3ea11391d796d7c1acc76f35aa2b2df2:a82d30120d016d343cbbd273504b76c589f6dfa4-0
  - id: diamond_database_builder
    environment: slurm_normal_container_resolv_fix
  # vSNP tools do not have corresponding BioContainers
  - id: vsnp_add_zero_coverage
    environment: slurm_normal_container_conda
  - id: vsnp_build_tables
    environment: slurm_normal_multicore_container_conda
  - id: vsnp_determine_ref_from_data
    environment: slurm_normal_container_conda
  - id: vsnp_get_snps
    environment: slurm_normal_multicore_container_conda
  - id: vsnp_statistics
    environment: slurm_normal_container_conda

limits:
  - type: registered_user_concurrent_jobs
    value: 8
  - type: environment_user_concurrent_jobs
    id: multicore
    value: 2

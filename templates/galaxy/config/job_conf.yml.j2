---

#
# Job runner plugin configuration
#
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    workers: 2
  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    workers: 4
    drmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1
    invalidjobexception_retries: 5
    internalexception_retries: 5
  frontera:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: frontera
    amqp_url: "{{ pulsar_message_queue_url }}"
    galaxy_url: "https://{{ galaxy_instance_hostname }}"
    persistence_directory: "{{ galaxy_root }}/var/pulsar_amqp_ack"
    amqp_acknowledge: true
    amqp_ack_republish_time: 1200
    amqp_consumer_timeout: 2.0
    amqp_publish_retry: true
    amqp_publish_retry_max_retries: 60


handling:
  assign:
    - db-skip-locked
  max_grab: 16
  ready_window_size: 32

execution:
  default: tpv_dispatcher
  environments:
    local:
      runner: local
    tpv_dispatcher:
      runner: dynamic
      type: python
      function: map_tool_to_destination
      rules_module: tpv.rules
      tpv_config_files:
        - https://raw.githubusercontent.com/galaxyproject/tpv-shared-database/main/tools.yml
        - "{{ galaxy_dynamic_job_rules_dir }}/total_perspective_vortex/default_tool.yml"
        - "{{ galaxy_dynamic_job_rules_dir }}/total_perspective_vortex/tools.yml"
        - "{{ galaxy_dynamic_job_rules_dir }}/total_perspective_vortex/destinations.yml"

    # legacy direct-mapped tools

    # single core, default (4GB) memory, non-container
    slurm_normal:
      runner: slurm
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: _JAVA_OPTIONS
          value: -Djava.io.tmpdir=$_GALAXY_JOB_TMP_DIR
    # single core, default (4GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_normal_container_venv:
      runner: slurm
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: _JAVA_OPTIONS
          value: -Djava.io.tmpdir=$_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"
    # default multi core (6), default (64GB) memory, singularity, galaxy venv (for tools that use galaxy lib)
    slurm_bigmem_multicore_container_venv:
      runner: slurm
      tags: [multicore]
      native_specification: --ntasks=6 --mem=65536 --partition=bigmem
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      outputs_to_working_directory: true
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.9--1"
      env:
        - name: LC_ALL
          value: C
        - name: TMPDIR
          value: $_GALAXY_JOB_TMP_DIR
        - name: TMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: TEMP
          value: $_GALAXY_JOB_TMP_DIR
        - name: _JAVA_OPTIONS
          value: -Djava.io.tmpdir=$_GALAXY_JOB_TMP_DIR
        - name: SINGULARITYENV_PREPEND_PATH
          value: "{{ galaxy_venv_dir }}/bin"

tools:
  # TODO: figure out how to make sure these do not run on Frontera.
  # param_value_from_file
  #__FILTER_FAILED_DATASETS__
  - id: Cut1
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  - id: Remove beginning1
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  - id: addValue
    environment: slurm_normal
    container_override: [{type: singularity, shell: '/bin/sh', identifier: "/cvmfs/singularity.galaxyproject.org/all/perl:5.22.0--9"}]
  # Other than the data source tools, these should still be runnable in a container
  - class: local
    environment: slurm_normal
  # Tools using a single core that need to run in the venv.
  - id: ucsc_table_direct1
    environment: slurm_normal_container_venv
  # Tools using multiple cores that need to run in the venv.
  - id: ncbi_makeblastdb
    environment: slurm_bigmem_multicore_container_venv
  # Tools that do not have corresponding BioContainers
  # Frontera

limits:
  - type: registered_user_concurrent_jobs
    value: 20
  # Per-user limits on queued/running jobs per-environment
  - type: environment_user_concurrent_jobs
    id: multicore
    value: 4
  # Total limits on queued/running jobs across all users
  - type: environment_total_concurrent_jobs
    id: frontera_development
    value: 1
  # Actual Frontera limits are 50 running and 200 pending
  - type: environment_total_concurrent_jobs
    id: frontera_limit
    value: 50
